{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec4152b-8b6a-4c84-acb5-400d80a5ea96",
   "metadata": {},
   "source": [
    "# Setting Up GPU / CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd548e29-f59f-44ad-a953-cf01800e87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7827beb-11db-4294-bda0-6ab6c1cb4b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activate GPU for faster training by clicking on 'Runtime' > 'Change runtime type' and then selecting GPU as the Hardware accelerator\n",
    "# Then check if GPU is available\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020c7ff4-80b0-4437-93ac-1ba65c156bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d3b15-e49a-4cdb-b037-7236adce12cf",
   "metadata": {},
   "source": [
    "# Installing and Loading Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d13fe-7f93-4b9c-872c-dd02c99b89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch pandas numpy datasets accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1456f56e-32ab-431e-bbd8-dbdf1f7a3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainerCallback, TrainingArguments\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ce72659-966c-4d00-9022-6d08511b29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "25cd3def-c3ef-4bb2-abb5-9c4175c70db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b0894f8-16d7-4476-8d51-203bb955188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2e364-7d85-4947-9813-5c8fca7f5031",
   "metadata": {},
   "source": [
    "# Importing The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106cc96-5250-41af-ae9c-9084a876d8f6",
   "metadata": {},
   "source": [
    "## Bitext - Customer Service Tagged Training Dataset\n",
    "\n",
    "### Overview\n",
    "\n",
    "This dataset can be used to train intent recognition models on Natural Language Understanding (NLU) platforms: LUIS, Dialogflow, Lex, RASA and any other NLU platform that accepts text as input.\n",
    "\n",
    "The training dataset contains 8,100 utterances (300 per intent), because most platforms limit the number of utterances that can be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574584a7-e74d-42d3-b43e-c0b1cc8624b4",
   "metadata": {},
   "source": [
    "## Cleaning up the dataset\n",
    "\n",
    "From the dataset, we are removing the categories which is not having less than 3 intents, which removes the following categories:\n",
    "- CANCELLATION_FEE\n",
    "- FEEDBACK\n",
    "- NEWSLETTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e60def6-8a80-4b8f-aedd-46e25129a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#navigate to parent directory\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "#setting the filenames\n",
    "training_file = os.path.join(parent_dir, 'data/train/Bitext_Sample_Customer_Service_Training_Dataset.csv')\n",
    "testing_file  = os.path.join(parent_dir, 'data/test/Bitext_Sample_Customer_Service_Testing_Dataset.csv')\n",
    "\n",
    "#opening the files\n",
    "training_df = pd.read_csv(training_file)\n",
    "testing_df  = pd.read_csv(testing_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9183b7bc-dc9e-4d2d-89e5-2b3fae3dfa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>intent</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>entity_value</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>end_offset</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how can I cancel purchase 113542617735902?</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>order_id</td>\n",
       "      <td>113542617735902</td>\n",
       "      <td>26.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>BIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can you help me canceling purchase 00004587345?</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>order_id</td>\n",
       "      <td>00004587345</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>BIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i want assistance to cancel purchase 732201349959</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>order_id</td>\n",
       "      <td>732201349959</td>\n",
       "      <td>37.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>BLQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i want assistance to cancel order 732201349959</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>order_id</td>\n",
       "      <td>732201349959</td>\n",
       "      <td>34.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>BQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't want my last item, help me cancel orde...</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>order_id</td>\n",
       "      <td>370795561790</td>\n",
       "      <td>48.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>BCLN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           utterance        intent  \\\n",
       "0         how can I cancel purchase 113542617735902?  cancel_order   \n",
       "1    can you help me canceling purchase 00004587345?  cancel_order   \n",
       "2  i want assistance to cancel purchase 732201349959  cancel_order   \n",
       "3     i want assistance to cancel order 732201349959  cancel_order   \n",
       "4  I don't want my last item, help me cancel orde...  cancel_order   \n",
       "\n",
       "  entity_type     entity_value  start_offset  end_offset category  tags  \n",
       "0    order_id  113542617735902          26.0        41.0    ORDER   BIL  \n",
       "1    order_id      00004587345          35.0        46.0    ORDER   BIL  \n",
       "2    order_id     732201349959          37.0        49.0    ORDER   BLQ  \n",
       "3    order_id     732201349959          34.0        46.0    ORDER    BQ  \n",
       "4    order_id     370795561790          48.0        60.0    ORDER  BCLN  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d840d2ce-09e8-46e3-962c-42113b37ecf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>intent</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>entity_value</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>end_offset</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I do not know how I can cancel purchase 00123842</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>order_id</td>\n",
       "      <td>00123842</td>\n",
       "      <td>40.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>BEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>help to cancel purchase 00004587345</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>order_id</td>\n",
       "      <td>00004587345</td>\n",
       "      <td>24.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>BL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cancelling purchase 00123842</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>order_id</td>\n",
       "      <td>00123842</td>\n",
       "      <td>20.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>BKL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cancel purchase 00004587345</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>order_id</td>\n",
       "      <td>00004587345</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>BKL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I  don't know how to cancel order 732201349959</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>order_id</td>\n",
       "      <td>732201349959</td>\n",
       "      <td>34.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>ORDER</td>\n",
       "      <td>BZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          utterance        intent entity_type  \\\n",
       "0  I do not know how I can cancel purchase 00123842  cancel_order    order_id   \n",
       "1               help to cancel purchase 00004587345  cancel_order    order_id   \n",
       "2                      cancelling purchase 00123842  cancel_order    order_id   \n",
       "3                       cancel purchase 00004587345  cancel_order    order_id   \n",
       "4    I  don't know how to cancel order 732201349959  cancel_order    order_id   \n",
       "\n",
       "   entity_value  start_offset  end_offset category tags  \n",
       "0      00123842          40.0        48.0    ORDER  BEL  \n",
       "1   00004587345          24.0        35.0    ORDER   BL  \n",
       "2      00123842          20.0        28.0    ORDER  BKL  \n",
       "3   00004587345          16.0        27.0    ORDER  BKL  \n",
       "4  732201349959          34.0        46.0    ORDER   BZ  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "56434e11-9afe-4f1f-bfec-8be619a27c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only categories with intents more than 2\n",
    "training_df = training_df[training_df[\"category\"].isin(['ACCOUNT', 'CONTACT', 'ORDER', 'PAYMENT', 'REFUND', 'SHIPPING_ADDRESS'])]\n",
    "# retain only categories with intents more than 2\n",
    "testing_df = testing_df[testing_df[\"category\"].isin(['ACCOUNT', 'CONTACT', 'ORDER', 'PAYMENT', 'REFUND', 'SHIPPING_ADDRESS'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c37d694-455b-4b9c-8dcb-0dcfb30d653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted tags\n",
    "# P - POLITENESS - ADDS FORMALITY\n",
    "# W - ANGER - DECREASE FORMALITY\n",
    "# C - CORDINATED - ADDS FORMALITY\n",
    "# Q - COLLOQUIAL - DECREASE FORMALITY\n",
    "# Z - ERRORS - DECREASE FORMALITY\n",
    "\n",
    "# when tags are empty, its informal and polite by default\n",
    "\n",
    "def remove_tags(tags):\n",
    "    new_tags = \"\"\n",
    "    for char in tags:\n",
    "        # if char not in ['B','I','L', 'N', 'G']:\n",
    "        if char in ['P', 'W', 'C', 'Q', 'Z']: # keeping only P,W,C,Q,Z\n",
    "            new_tags += char\n",
    "    return new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3571c0c0-e3b6-4442-a496-48ba6acd3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preprocess(input_df):\n",
    "    # drop other columns\n",
    "    # keep only utterance, intent and tags\n",
    "    df = input_df[['utterance','intent', 'tags']]\n",
    "\n",
    "    # removing not-needed tags from dataframe\n",
    "    df['tags'] = df['tags'].apply(remove_tags)\n",
    "    \n",
    "    df['encoded_intent'] = df['intent'].astype('category').cat.codes\n",
    "    df['labels'] = df['tags'].apply(lambda x: [1 if letter in x else 0 for letter in 'PWCQZ'])\n",
    "\n",
    "    columns_to_drop = ['intent', 'tags']\n",
    "    new_df = df.drop(columns_to_drop, axis=1)\n",
    "    # new_df.head()\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65b9b7af-ce4f-4f80-8e86-7651bddbf84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18223/648801469.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tags'] = df['tags'].apply(remove_tags)\n",
      "/tmp/ipykernel_18223/648801469.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['encoded_intent'] = df['intent'].astype('category').cat.codes\n",
      "/tmp/ipykernel_18223/648801469.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['tags'].apply(lambda x: [1 if letter in x else 0 for letter in 'PWCQZ'])\n",
      "/tmp/ipykernel_18223/648801469.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tags'] = df['tags'].apply(remove_tags)\n",
      "/tmp/ipykernel_18223/648801469.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['encoded_intent'] = df['intent'].astype('category').cat.codes\n",
      "/tmp/ipykernel_18223/648801469.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['tags'].apply(lambda x: [1 if letter in x else 0 for letter in 'PWCQZ'])\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset_preprocess(training_df)\n",
    "test_data  = dataset_preprocess(testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58bf3b1a-d9c1-430e-9cc3-01666aecaeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>encoded_intent</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how can I cancel purchase 113542617735902?</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can you help me canceling purchase 00004587345?</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i want assistance to cancel purchase 732201349959</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i want assistance to cancel order 732201349959</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't want my last item, help me cancel orde...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           utterance  encoded_intent  \\\n",
       "0         how can I cancel purchase 113542617735902?               0   \n",
       "1    can you help me canceling purchase 00004587345?               0   \n",
       "2  i want assistance to cancel purchase 732201349959               0   \n",
       "3     i want assistance to cancel order 732201349959               0   \n",
       "4  I don't want my last item, help me cancel orde...               0   \n",
       "\n",
       "            labels  \n",
       "0  [0, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0]  \n",
       "2  [0, 0, 0, 1, 0]  \n",
       "3  [0, 0, 0, 1, 0]  \n",
       "4  [0, 0, 1, 0, 0]  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "943d375a-a46a-4007-8707-47984722351c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>encoded_intent</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I do not know how I can cancel purchase 00123842</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>help to cancel purchase 00004587345</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cancelling purchase 00123842</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cancel purchase 00004587345</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I  don't know how to cancel order 732201349959</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          utterance  encoded_intent  \\\n",
       "0  I do not know how I can cancel purchase 00123842               0   \n",
       "1               help to cancel purchase 00004587345               0   \n",
       "2                      cancelling purchase 00123842               0   \n",
       "3                       cancel purchase 00004587345               0   \n",
       "4    I  don't know how to cancel order 732201349959               0   \n",
       "\n",
       "            labels  \n",
       "0  [0, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0]  \n",
       "3  [0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 1]  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102396c-7e63-4ce8-940d-ee2b50fcc48e",
   "metadata": {},
   "source": [
    "# PreProcessing and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12faa2fb-1d16-48c5-9c21-3a6a37f110d3",
   "metadata": {},
   "source": [
    "We load the DistilBERT backbone model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e4275cdf-6a69-4725-a510-075858bd8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DistilBERT tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc579a-b49d-4e61-aa0a-4ae8cc6fe57c",
   "metadata": {},
   "source": [
    "Tokenize the text (input) and represent the label (output) as a (19+5)-24-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f7d62308-e00d-4154-a4c8-8d5851d27a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the text inputs for the model\n",
    "def preprocess_function(examples):\n",
    "\n",
    "    intent_labels = np.zeros(19)\n",
    "\n",
    "    # assigning the labels for intents\n",
    "    for i in range(len(intent_labels)):\n",
    "      intent_value = examples[\"encoded_intent\"]\n",
    "      intent_labels[intent_value] = 1\n",
    "\n",
    "    # tags labels\n",
    "    tags = np.array(examples[\"labels\"])\n",
    "\n",
    "    labels = np.append(intent_labels, tags)\n",
    "\n",
    "    examples = tokenizer(examples[\"utterance\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    examples[\"label\"] = labels\n",
    "\n",
    "    # print(len(examples[\"label\"]))\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8b7dfb25-0acb-4520-b059-77257cd49ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2215, 5375, 2000, 17542, 2344, 6421, 19317, 24096, 22022, 2683, 2683, 28154, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 1.])}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the preprocess_function\n",
    "preprocess_function({\n",
    "    \"utterance\": \"i want assistance to cancel order 732201349959\",\n",
    "    \"encoded_intent\": 0,\n",
    "    \"labels\": [1, 0, 1, 0, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d961e399-c57f-4216-bc0e-165cc7869b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (4580, 3)\n",
      "TEST Dataset: (565, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dcd18f84-7751-4427-80ab-64db6d0e387c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██| 4580/4580 [00:00<00:00, 6610.88 examples/s]\n",
      "Map: 100%|████| 565/565 [00:00<00:00, 6608.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the text inputs for the model\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function)\n",
    "tokenized_test = test_dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a91a2d-5a65-4dba-a8e0-05a496405481",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17f7a6-7b24-4a6c-8d8a-a768f1e5d98b",
   "metadata": {},
   "source": [
    "## Fine-Tuning DistilBERT\n",
    "\n",
    "Let’s start by importing the necessary modules and defining some constants for hyperparameters like the base model, learning rate, batch size, number of epochs for training and max length of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "988ac74e-eafe-48ec-8f3e-b88d5047cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the training parameters\n",
    "\n",
    "LEARNING_RATE = 1e-04\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 20\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1d9339c0-135d-4f9e-a0ac-82dbe1d140e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data_collector to convert our samples to PyTorch tensors and concatenate them with the correct amount of padding\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4f8e6cff-2479-4463-85bf-c6e2417d078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=24, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define DistilBERT as our base model:\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=24) #total output features = 19+5 = 24\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54718969-fbbb-4ae5-a021-f64a69b570c9",
   "metadata": {},
   "source": [
    "## OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603d372-a541-4cc8-9e09-c4e6577cfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1daf4204-5754-4029-8ec3-d917f821979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_optimizer as optim\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d5015351-809a-419e-8d2f-c5d7e8870e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d346bef-e8a3-4ada-b130-0ee0f923e5f1",
   "metadata": {},
   "source": [
    "# Prepare the Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08b38f-ccf1-4a5b-8911-44709c360c35",
   "metadata": {},
   "source": [
    "During the training phase, the trained model will compute the logits of each input. The Huggingface trainer object will deduce the predicted vectors from these logits, compare these predicted vectors with the labels, and return metrics like accuracy, precision, recall and f1-score on the whole validation set.\n",
    "\n",
    "- for multiclass classification, the predicted vector is deduced by putting 1 to the class with the highest logit and 0 to all the other classes. (Equivalently, if we compute softmax on all the classes, the class with the highest logit will get the highest probability).\n",
    "\n",
    "- for multilabel classification, we choose the threshold = 0 (with the sigmoid function this threshold corresponds to the probability = 0.5). The predicted vector is deduced by putting 1 to all non-negative logits and 0 to all negative logits (which means the input with a probability >= 0.5 belongs to the corresponding class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7f04ecd8-ca9b-4852-b498-114906bd3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_INDICES = range(0,19)\n",
    "TAGS_INDICES   = range(19,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b7e113fc-d722-4169-857b-65c1fed47ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply the two logics for multiclass columns and multilabel columns\n",
    "# Apply two logics for Multiclass Columns and Multilabel Columns\n",
    "def get_preds_from_logits(logits):\n",
    "    ret = np.zeros(logits.shape)\n",
    "\n",
    "    # The first 19 columns are for customer intents. They should be handled with a multiclass approach\n",
    "    # i.e. we fill 1 to the class with highest probability, and 0 into the other columns\n",
    "    best_class = np.argmax(logits[:, INTENT_INDICES], axis=-1)\n",
    "    ret[list(range(len(ret))), best_class] = 1\n",
    "\n",
    "    # The other columns are for register tags. They should be handled with multilabel approach.\n",
    "    # i.e. we fill 1 to every class whose score is higher than some threshold\n",
    "    # In this example, we choose that threshold = 0\n",
    "    ret[:, TAGS_INDICES] = (logits[:, TAGS_INDICES] >= 0).astype(int)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c8c62bd-7fde-4562-a0fd-ec91c05dbe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.50183952,  1.80285723,  0.92797577,  0.39463394, -1.37592544,\n",
       "        -1.37602192, -1.76766555,  1.46470458,  0.40446005,  0.83229031,\n",
       "        -1.91766202,  1.87963941,  1.32977056, -1.15064356, -1.27270013,\n",
       "        -1.26638196, -0.78303103,  0.09902573, -0.27221993, -0.83508344,\n",
       "         0.44741158, -1.44202456, -0.83142141, -0.53455263]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s look at an example by generating a random 30-dimensional vector whose columns are between -2 and 2\n",
    "example = np.random.uniform(-2, 2, (1, 24))\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5d900390-cf5a-4cdc-844a-f946af188dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the predicted vector\n",
    "get_preds_from_logits(example)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162be63-f88d-4afe-a75e-101815d6c1c2",
   "metadata": {},
   "source": [
    "There is only 1 output among the first 19 classes, and multiple outputs from classes 19 till end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c16afa7c-eae9-4833-b946-6652ccea3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    final_metrics = {}\n",
    "\n",
    "    # Deduce predictions from logits\n",
    "    predictions = get_preds_from_logits(logits)\n",
    "\n",
    "    # Get f1 metrics for global scoring. Notice that f1_micro = accuracy\n",
    "    final_metrics[\"f1_micro_for_intents\"] = f1_score(labels[:, INTENT_INDICES], predictions[:, INTENT_INDICES], average=\"micro\")\n",
    "    final_metrics[\"f1_macro_for_intents\"] = f1_score(labels[:, INTENT_INDICES], predictions[:, INTENT_INDICES], average=\"macro\")\n",
    "\n",
    "    # Get f1 metrics for causes\n",
    "    final_metrics[\"f1_micro_for_tags\"] = f1_score(labels[:, TAGS_INDICES], predictions[:, TAGS_INDICES], average=\"micro\")\n",
    "    final_metrics[\"f1_macro_for_tags\"] = f1_score(labels[:, TAGS_INDICES], predictions[:, TAGS_INDICES], average=\"macro\")\n",
    "\n",
    "    # The global f1_metrics\n",
    "    final_metrics[\"f1_micro\"] = f1_score(labels, predictions, average=\"micro\")\n",
    "    final_metrics[\"f1_macro\"] = f1_score(labels, predictions, average=\"macro\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification report for intents: \")\n",
    "    print(classification_report(labels[:, INTENT_INDICES], predictions[:, INTENT_INDICES], zero_division=0))\n",
    "    print(\"Classification report for tags: \")\n",
    "    print(classification_report(labels[:, TAGS_INDICES], predictions[:, TAGS_INDICES], zero_division=0))\n",
    "\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fe7fe-b783-4935-92ac-7849359f5a86",
   "metadata": {},
   "source": [
    "# LOSS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5af0cc7a-6974-4cbe-b2f8-33bd9d9b088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskClassificationTrainer(Trainer):\n",
    "    def __init__(self, group_weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.group_weights = group_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "\n",
    "        intent_loss = torch.nn.functional.cross_entropy(logits[:, INTENT_INDICES], labels[:, INTENT_INDICES])\n",
    "        tags_loss = torch.nn.functional.binary_cross_entropy_with_logits(logits[:, TAGS_INDICES], labels[:, TAGS_INDICES])\n",
    "\n",
    "        loss = self.group_weights[0] * intent_loss + self.group_weights[1] * tags_loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fa9fa4f1-55cb-4fd3-891e-e22533421e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print epoch number at each step\n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n",
    "        print(f\"Epoch {state.epoch}: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c231e0-32cc-4120-954a-a3f0277abe05",
   "metadata": {},
   "source": [
    "# TRAINER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738434f-f79c-4093-8f54-7ed6f677bd56",
   "metadata": {},
   "source": [
    "### Accelerator Workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "954bd661-99c4-404d-8e16-cccca4d99158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.31.0', '0.21.0')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dced01ab-58bb-4b1c-a06d-55cc39fc2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new Trainer with all the objects we constructed so far\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    optim='adamw_torch',\n",
    ")\n",
    "\n",
    "trainer = MultiTaskClassificationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrinterCallback],\n",
    "    data_collator=data_collator,\n",
    "    group_weights=(0.8, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "712ea0ff-60ce-4523-b6bd-46da8072b103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22900' max='22900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22900/22900 32:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro For Intents</th>\n",
       "      <th>F1 Macro For Intents</th>\n",
       "      <th>F1 Micro For Tags</th>\n",
       "      <th>F1 Macro For Tags</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.824100</td>\n",
       "      <td>0.792128</td>\n",
       "      <td>0.982301</td>\n",
       "      <td>0.982989</td>\n",
       "      <td>0.783476</td>\n",
       "      <td>0.805285</td>\n",
       "      <td>0.906114</td>\n",
       "      <td>0.945967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.584400</td>\n",
       "      <td>0.537370</td>\n",
       "      <td>0.984071</td>\n",
       "      <td>0.983410</td>\n",
       "      <td>0.860526</td>\n",
       "      <td>0.914190</td>\n",
       "      <td>0.934392</td>\n",
       "      <td>0.968989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.535710</td>\n",
       "      <td>0.984071</td>\n",
       "      <td>0.984037</td>\n",
       "      <td>0.870801</td>\n",
       "      <td>0.923494</td>\n",
       "      <td>0.938025</td>\n",
       "      <td>0.971424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0.582283</td>\n",
       "      <td>0.985841</td>\n",
       "      <td>0.983820</td>\n",
       "      <td>0.867439</td>\n",
       "      <td>0.888025</td>\n",
       "      <td>0.937598</td>\n",
       "      <td>0.963863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.534663</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991269</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.934798</td>\n",
       "      <td>0.949580</td>\n",
       "      <td>0.979505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.596947</td>\n",
       "      <td>0.989381</td>\n",
       "      <td>0.988654</td>\n",
       "      <td>0.875800</td>\n",
       "      <td>0.919637</td>\n",
       "      <td>0.942962</td>\n",
       "      <td>0.974275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.581293</td>\n",
       "      <td>0.987611</td>\n",
       "      <td>0.987448</td>\n",
       "      <td>0.881529</td>\n",
       "      <td>0.923157</td>\n",
       "      <td>0.944125</td>\n",
       "      <td>0.974054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.415200</td>\n",
       "      <td>0.593938</td>\n",
       "      <td>0.992920</td>\n",
       "      <td>0.992941</td>\n",
       "      <td>0.881664</td>\n",
       "      <td>0.927493</td>\n",
       "      <td>0.947867</td>\n",
       "      <td>0.979306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.329600</td>\n",
       "      <td>0.491463</td>\n",
       "      <td>0.987611</td>\n",
       "      <td>0.987333</td>\n",
       "      <td>0.894207</td>\n",
       "      <td>0.941404</td>\n",
       "      <td>0.949064</td>\n",
       "      <td>0.977764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>0.660739</td>\n",
       "      <td>0.996460</td>\n",
       "      <td>0.996430</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.937145</td>\n",
       "      <td>0.951042</td>\n",
       "      <td>0.984079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>0.642227</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991192</td>\n",
       "      <td>0.893350</td>\n",
       "      <td>0.937764</td>\n",
       "      <td>0.950701</td>\n",
       "      <td>0.980061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.691267</td>\n",
       "      <td>0.996460</td>\n",
       "      <td>0.995584</td>\n",
       "      <td>0.893401</td>\n",
       "      <td>0.938897</td>\n",
       "      <td>0.954119</td>\n",
       "      <td>0.983774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.213900</td>\n",
       "      <td>0.782938</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.990691</td>\n",
       "      <td>0.887237</td>\n",
       "      <td>0.937322</td>\n",
       "      <td>0.947858</td>\n",
       "      <td>0.979572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.848587</td>\n",
       "      <td>0.994690</td>\n",
       "      <td>0.994421</td>\n",
       "      <td>0.884422</td>\n",
       "      <td>0.936078</td>\n",
       "      <td>0.949117</td>\n",
       "      <td>0.982266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.812728</td>\n",
       "      <td>0.994690</td>\n",
       "      <td>0.994220</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.936633</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.982222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>0.941449</td>\n",
       "      <td>0.996460</td>\n",
       "      <td>0.996402</td>\n",
       "      <td>0.884076</td>\n",
       "      <td>0.934552</td>\n",
       "      <td>0.950392</td>\n",
       "      <td>0.983516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.993435</td>\n",
       "      <td>0.994690</td>\n",
       "      <td>0.994456</td>\n",
       "      <td>0.878981</td>\n",
       "      <td>0.931997</td>\n",
       "      <td>0.947258</td>\n",
       "      <td>0.981444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>1.034396</td>\n",
       "      <td>0.994690</td>\n",
       "      <td>0.994638</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.938430</td>\n",
       "      <td>0.950803</td>\n",
       "      <td>0.982928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>0.975685</td>\n",
       "      <td>0.996460</td>\n",
       "      <td>0.996402</td>\n",
       "      <td>0.890819</td>\n",
       "      <td>0.938605</td>\n",
       "      <td>0.952479</td>\n",
       "      <td>0.984361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.994417</td>\n",
       "      <td>0.996460</td>\n",
       "      <td>0.996402</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.940629</td>\n",
       "      <td>0.953512</td>\n",
       "      <td>0.984782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        29\n",
      "           1       1.00      0.90      0.95        21\n",
      "           2       0.97      1.00      0.98        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      0.97      0.99        40\n",
      "           5       1.00      0.93      0.96        41\n",
      "           6       0.92      1.00      0.96        36\n",
      "           7       1.00      0.96      0.98        27\n",
      "           8       0.98      1.00      0.99        40\n",
      "           9       1.00      0.96      0.98        28\n",
      "          10       0.96      1.00      0.98        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       1.00      0.96      0.98        27\n",
      "          18       0.97      1.00      0.98        28\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       565\n",
      "   macro avg       0.98      0.98      0.98       565\n",
      "weighted avg       0.98      0.98      0.98       565\n",
      " samples avg       0.98      0.98      0.98       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92        26\n",
      "           1       1.00      0.49      0.65        35\n",
      "           2       1.00      0.87      0.93        46\n",
      "           3       0.95      0.62      0.75       198\n",
      "           4       1.00      0.63      0.77       115\n",
      "\n",
      "   micro avg       0.98      0.65      0.78       420\n",
      "   macro avg       0.98      0.70      0.81       420\n",
      "weighted avg       0.98      0.65      0.78       420\n",
      " samples avg       0.38      0.33      0.35       420\n",
      "\n",
      "Epoch 2.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       0.97      1.00      0.98        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      0.97      0.99        36\n",
      "           7       1.00      0.96      0.98        27\n",
      "           8       0.97      0.97      0.97        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       0.97      0.97      0.97        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      0.95      0.97        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      0.97      0.98        32\n",
      "          16       0.91      1.00      0.95        21\n",
      "          17       0.90      1.00      0.95        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       565\n",
      "   macro avg       0.98      0.98      0.98       565\n",
      "weighted avg       0.99      0.98      0.98       565\n",
      " samples avg       0.98      0.98      0.98       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.91      0.95        46\n",
      "           3       0.93      0.69      0.79       198\n",
      "           4       0.99      0.77      0.86       115\n",
      "\n",
      "   micro avg       0.96      0.78      0.86       420\n",
      "   macro avg       0.97      0.87      0.91       420\n",
      "weighted avg       0.96      0.78      0.86       420\n",
      " samples avg       0.42      0.38      0.39       420\n",
      "\n",
      "Epoch 3.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        29\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       0.97      1.00      0.98        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      0.97      0.99        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      0.93      0.96        27\n",
      "           8       0.95      1.00      0.98        40\n",
      "           9       1.00      0.96      0.98        28\n",
      "          10       0.90      1.00      0.95        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.95      1.00      0.97        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      0.95      0.98        21\n",
      "          17       0.96      1.00      0.98        27\n",
      "          18       1.00      0.93      0.96        28\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       565\n",
      "   macro avg       0.99      0.98      0.98       565\n",
      "weighted avg       0.99      0.98      0.98       565\n",
      " samples avg       0.98      0.98      0.98       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.91      0.95        46\n",
      "           3       0.91      0.75      0.83       198\n",
      "           4       0.97      0.74      0.84       115\n",
      "\n",
      "   micro avg       0.95      0.80      0.87       420\n",
      "   macro avg       0.98      0.88      0.92       420\n",
      "weighted avg       0.95      0.80      0.87       420\n",
      " samples avg       0.40      0.39      0.39       420\n",
      "\n",
      "Epoch 4.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        29\n",
      "           1       0.91      0.95      0.93        21\n",
      "           2       0.97      1.00      0.98        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      0.97      0.99        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       0.98      1.00      0.99        40\n",
      "           9       1.00      0.96      0.98        28\n",
      "          10       0.96      1.00      0.98        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      0.86      0.92        21\n",
      "          17       0.96      1.00      0.98        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.98      0.98       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.65      0.77        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       0.98      0.98      0.98        46\n",
      "           3       0.90      0.77      0.83       198\n",
      "           4       0.98      0.77      0.86       115\n",
      "\n",
      "   micro avg       0.94      0.80      0.87       420\n",
      "   macro avg       0.96      0.83      0.89       420\n",
      "weighted avg       0.94      0.80      0.86       420\n",
      " samples avg       0.42      0.39      0.40       420\n",
      "\n",
      "Epoch 5.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       0.97      1.00      0.98        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      0.97      0.99        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       0.97      1.00      0.99        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       0.96      1.00      0.98        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      0.96      0.98        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      0.97      0.98        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       1.00      1.00      1.00        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.93      0.97        46\n",
      "           3       0.95      0.77      0.85       198\n",
      "           4       0.98      0.77      0.86       115\n",
      "\n",
      "   micro avg       0.97      0.82      0.89       420\n",
      "   macro avg       0.99      0.89      0.93       420\n",
      "weighted avg       0.97      0.82      0.89       420\n",
      " samples avg       0.41      0.39      0.40       420\n",
      "\n",
      "Epoch 6.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       0.98      1.00      0.99        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       0.96      1.00      0.98        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       0.96      0.96      0.96        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      0.95      0.98        21\n",
      "          17       0.96      0.96      0.96        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        26\n",
      "           1       0.92      1.00      0.96        35\n",
      "           2       1.00      0.93      0.97        46\n",
      "           3       0.92      0.75      0.83       198\n",
      "           4       0.98      0.77      0.86       115\n",
      "\n",
      "   micro avg       0.95      0.81      0.88       420\n",
      "   macro avg       0.96      0.89      0.92       420\n",
      "weighted avg       0.95      0.81      0.87       420\n",
      " samples avg       0.42      0.39      0.40       420\n",
      "\n",
      "Epoch 7.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       0.95      1.00      0.98        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      0.94      0.97        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      0.96      0.98        28\n",
      "          10       1.00      0.92      0.96        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       0.93      1.00      0.96        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       0.95      1.00      0.98        21\n",
      "          17       1.00      1.00      1.00        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.94        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.98      0.99        46\n",
      "           3       0.90      0.80      0.85       198\n",
      "           4       0.99      0.73      0.84       115\n",
      "\n",
      "   micro avg       0.95      0.82      0.88       420\n",
      "   macro avg       0.98      0.88      0.92       420\n",
      "weighted avg       0.95      0.82      0.88       420\n",
      " samples avg       0.42      0.40      0.40       420\n",
      "\n",
      "Epoch 8.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       0.98      1.00      0.99        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       0.97      1.00      0.99        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       1.00      0.96      0.98        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      0.96      0.98        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       0.96      1.00      0.98        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.98      0.99        46\n",
      "           3       0.97      0.74      0.84       198\n",
      "           4       0.96      0.77      0.85       115\n",
      "\n",
      "   micro avg       0.97      0.81      0.88       420\n",
      "   macro avg       0.98      0.89      0.93       420\n",
      "weighted avg       0.97      0.81      0.88       420\n",
      " samples avg       0.41      0.39      0.39       420\n",
      "\n",
      "Epoch 9.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       0.93      1.00      0.96        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      0.97      0.99        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      0.96      0.98        28\n",
      "          10       1.00      0.88      0.94        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       0.96      1.00      0.98        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       0.95      1.00      0.98        21\n",
      "          17       0.96      1.00      0.98        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.96      0.98        46\n",
      "           3       0.89      0.80      0.85       198\n",
      "           4       1.00      0.79      0.88       115\n",
      "\n",
      "   micro avg       0.95      0.85      0.89       420\n",
      "   macro avg       0.98      0.91      0.94       420\n",
      "weighted avg       0.95      0.85      0.89       420\n",
      " samples avg       0.42      0.41      0.41       420\n",
      "\n",
      "Epoch 10.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        29\n",
      "           1       0.95      1.00      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       1.00      1.00      1.00        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       565\n",
      "   macro avg       1.00      1.00      1.00       565\n",
      "weighted avg       1.00      1.00      1.00       565\n",
      " samples avg       1.00      1.00      1.00       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.98      0.99        46\n",
      "           3       0.89      0.79      0.84       198\n",
      "           4       0.99      0.76      0.86       115\n",
      "\n",
      "   micro avg       0.95      0.83      0.89       420\n",
      "   macro avg       0.98      0.91      0.94       420\n",
      "weighted avg       0.95      0.83      0.88       420\n",
      " samples avg       0.42      0.40      0.41       420\n",
      "\n",
      "Epoch 11.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      0.95      0.97        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       0.93      1.00      0.96        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       0.93      1.00      0.96        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.93      0.97        46\n",
      "           3       0.90      0.82      0.86       198\n",
      "           4       0.97      0.78      0.87       115\n",
      "\n",
      "   micro avg       0.94      0.85      0.89       420\n",
      "   macro avg       0.97      0.91      0.94       420\n",
      "weighted avg       0.94      0.85      0.89       420\n",
      " samples avg       0.43      0.41      0.42       420\n",
      "\n",
      "Epoch 12.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       0.96      1.00      0.98        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      1.00      1.00        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       1.00      1.00      1.00        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      0.95      0.98        21\n",
      "          17       1.00      1.00      1.00        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       565\n",
      "   macro avg       1.00      0.99      1.00       565\n",
      "weighted avg       1.00      1.00      1.00       565\n",
      " samples avg       1.00      1.00      1.00       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       0.98      0.98      0.98        46\n",
      "           3       0.91      0.80      0.85       198\n",
      "           4       1.00      0.76      0.86       115\n",
      "\n",
      "   micro avg       0.96      0.84      0.89       420\n",
      "   macro avg       0.98      0.91      0.94       420\n",
      "weighted avg       0.96      0.84      0.89       420\n",
      " samples avg       0.42      0.41      0.41       420\n",
      "\n",
      "Epoch 13.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       0.97      1.00      0.98        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      0.97      0.99        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       0.96      1.00      0.98        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      0.96      0.98        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      0.95      0.98        21\n",
      "          17       1.00      1.00      1.00        27\n",
      "          18       0.97      1.00      0.98        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.96      0.98        46\n",
      "           3       0.85      0.83      0.84       198\n",
      "           4       1.00      0.77      0.87       115\n",
      "\n",
      "   micro avg       0.93      0.85      0.89       420\n",
      "   macro avg       0.97      0.91      0.94       420\n",
      "weighted avg       0.93      0.85      0.89       420\n",
      " samples avg       0.42      0.41      0.42       420\n",
      "\n",
      "Epoch 14.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       0.96      0.96      0.96        27\n",
      "          18       0.97      1.00      0.98        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.98      0.99        46\n",
      "           3       0.87      0.81      0.84       198\n",
      "           4       0.99      0.75      0.85       115\n",
      "\n",
      "   micro avg       0.94      0.84      0.88       420\n",
      "   macro avg       0.97      0.91      0.94       420\n",
      "weighted avg       0.94      0.84      0.88       420\n",
      " samples avg       0.42      0.41      0.41       420\n",
      "\n",
      "Epoch 15.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       0.97      1.00      0.98        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      0.97      0.99        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      0.96      0.98        28\n",
      "          10       0.96      1.00      0.98        26\n",
      "          11       1.00      1.00      1.00        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       1.00      1.00      1.00        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       0.96      1.00      0.98        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.98      0.99        46\n",
      "           3       0.89      0.80      0.84       198\n",
      "           4       0.99      0.75      0.85       115\n",
      "\n",
      "   micro avg       0.95      0.83      0.89       420\n",
      "   macro avg       0.98      0.90      0.94       420\n",
      "weighted avg       0.95      0.83      0.88       420\n",
      " samples avg       0.42      0.41      0.41       420\n",
      "\n",
      "Epoch 16.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       1.00      1.00      1.00        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       565\n",
      "   macro avg       1.00      1.00      1.00       565\n",
      "weighted avg       1.00      1.00      1.00       565\n",
      " samples avg       1.00      1.00      1.00       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.96      0.98        46\n",
      "           3       0.90      0.78      0.84       198\n",
      "           4       0.99      0.76      0.86       115\n",
      "\n",
      "   micro avg       0.95      0.83      0.88       420\n",
      "   macro avg       0.98      0.90      0.93       420\n",
      "weighted avg       0.95      0.83      0.88       420\n",
      " samples avg       0.42      0.40      0.41       420\n",
      "\n",
      "Epoch 17.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       0.97      1.00      0.98        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      0.96      0.98        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       0.96      1.00      0.98        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.96      0.98        46\n",
      "           3       0.89      0.77      0.83       198\n",
      "           4       0.98      0.76      0.85       115\n",
      "\n",
      "   micro avg       0.95      0.82      0.88       420\n",
      "   macro avg       0.97      0.90      0.93       420\n",
      "weighted avg       0.94      0.82      0.88       420\n",
      " samples avg       0.42      0.40      0.40       420\n",
      "\n",
      "Epoch 18.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      0.97      0.99        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       0.96      1.00      0.98        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       0.96      1.00      0.98        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       565\n",
      "   macro avg       0.99      0.99      0.99       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      " samples avg       0.99      0.99      0.99       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.98      0.99        46\n",
      "           3       0.88      0.81      0.85       198\n",
      "           4       0.96      0.77      0.86       115\n",
      "\n",
      "   micro avg       0.93      0.85      0.89       420\n",
      "   macro avg       0.97      0.91      0.94       420\n",
      "weighted avg       0.93      0.85      0.89       420\n",
      " samples avg       0.42      0.41      0.41       420\n",
      "\n",
      "Epoch 19.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       1.00      1.00      1.00        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       565\n",
      "   macro avg       1.00      1.00      1.00       565\n",
      "weighted avg       1.00      1.00      1.00       565\n",
      " samples avg       1.00      1.00      1.00       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.98      0.99        46\n",
      "           3       0.89      0.83      0.86       198\n",
      "           4       0.94      0.77      0.85       115\n",
      "\n",
      "   micro avg       0.93      0.85      0.89       420\n",
      "   macro avg       0.96      0.92      0.94       420\n",
      "weighted avg       0.93      0.85      0.89       420\n",
      " samples avg       0.43      0.42      0.42       420\n",
      "\n",
      "Epoch 20.0: \n",
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       1.00      1.00      1.00        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       565\n",
      "   macro avg       1.00      1.00      1.00       565\n",
      "weighted avg       1.00      1.00      1.00       565\n",
      " samples avg       1.00      1.00      1.00       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.98      0.99        46\n",
      "           3       0.89      0.82      0.86       198\n",
      "           4       0.94      0.79      0.86       115\n",
      "\n",
      "   micro avg       0.93      0.86      0.89       420\n",
      "   macro avg       0.97      0.92      0.94       420\n",
      "weighted avg       0.93      0.86      0.89       420\n",
      " samples avg       0.43      0.42      0.42       420\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22900, training_loss=0.3399937641360354, metrics={'train_runtime': 1940.0045, 'train_samples_per_second': 47.216, 'train_steps_per_second': 11.804, 'total_flos': 3034693590220800.0, 'train_loss': 0.3399937641360354, 'epoch': 20.0})"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fa723d2d-8c02-4c43-b4c3-fa6ba9fea237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [142/142 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for intents: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        29\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      1.00      1.00        41\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        27\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        28\n",
      "          10       1.00      1.00      1.00        26\n",
      "          11       1.00      0.97      0.99        38\n",
      "          12       1.00      1.00      1.00        27\n",
      "          13       1.00      1.00      1.00        20\n",
      "          14       0.97      1.00      0.99        36\n",
      "          15       1.00      1.00      1.00        32\n",
      "          16       1.00      1.00      1.00        21\n",
      "          17       1.00      1.00      1.00        27\n",
      "          18       1.00      1.00      1.00        28\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       565\n",
      "   macro avg       1.00      1.00      1.00       565\n",
      "weighted avg       1.00      1.00      1.00       565\n",
      " samples avg       1.00      1.00      1.00       565\n",
      "\n",
      "Classification report for tags: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        26\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.98      0.99        46\n",
      "           3       0.89      0.82      0.86       198\n",
      "           4       0.94      0.79      0.86       115\n",
      "\n",
      "   micro avg       0.93      0.86      0.89       420\n",
      "   macro avg       0.97      0.92      0.94       420\n",
      "weighted avg       0.93      0.86      0.89       420\n",
      " samples avg       0.43      0.42      0.42       420\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9944170713424683,\n",
       " 'eval_f1_micro_for_intents': 0.9964601769911504,\n",
       " 'eval_f1_macro_for_intents': 0.9964015073728877,\n",
       " 'eval_f1_micro_for_tags': 0.8933002481389578,\n",
       " 'eval_f1_macro_for_tags': 0.9406289199336294,\n",
       " 'eval_f1_micro': 0.953512396694215,\n",
       " 'eval_f1_macro': 0.9847822183230424,\n",
       " 'eval_runtime': 2.3751,\n",
       " 'eval_samples_per_second': 237.883,\n",
       " 'eval_steps_per_second': 59.786,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "38516101-e701-4c84-8287-0dbd39ad1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for future use\n",
    "model.save_pretrained('./models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75a4c1-2e06-46e0-88f0-105cc2a262f6",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "61983f96-d9c8-4bb3-b7e0-e5ee57fca7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "81d1eb73-38d5-4e82-a669-4559b49f7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load the custom pretrained model\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "grandparent_directory = os.path.dirname(parent_directory)\n",
    "\n",
    "#navigate to model directory\n",
    "# model_path = os.path.join(grandparent_directory, 'models')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('./models')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bd4c61a3-2e11-4112-8fb6-f8bc067decff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "35ccb950-e5f2-4da1-8620-75e8f64ab021",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts =  \"I need help cancelling my order.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "71bf52bc-93a3-4d93-97f1-8959609d7f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text\n",
    "encoded_input = tokenizer(input_texts,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=128,\n",
    "                          return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0e0f0d2d-98ce-4a12-b765-849af0e94e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model to predict under the format of logits of 24 classes\n",
    "logits = model(**encoded_input).logits.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "42e60cd4-f59b-453c-9304-97a89e613fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the result\n",
    "preds = get_preds_from_logits(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "65dc438f-8a88-426b-b0b6-edcb2970cfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f6c53a-61a6-45e8-bb27-a5c2479e67d5",
   "metadata": {},
   "source": [
    "## TRANSLATING OUTPUTS FROM OUTPUT LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "65d88329-6d64-4d32-bbd4-6c25faa6f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DICTIONARY\n",
    "\n",
    "intents_map = {\n",
    "    0: 'cancel_order',\n",
    "    1: 'change_order',\n",
    "    2: 'change_shipping_address',\n",
    "    3: 'check_payment_methods',\n",
    "    4: 'check_refund_policy',\n",
    "    5: 'contact_customer_service',\n",
    "    6: 'contact_human_agent',\n",
    "    7: 'create_account',\n",
    "    8: 'delete_account',\n",
    "    9: 'edit_account',\n",
    "   10: 'get_refund',\n",
    "   11: 'payment_issue',\n",
    "   12: 'place_order',\n",
    "   13: 'recover_password',\n",
    "   14: 'registration_problems',\n",
    "   15: 'set_up_shipping_address',\n",
    "   16: 'switch_account',\n",
    "   17: 'track_order',\n",
    "   18: 'track_refund'\n",
    "}\n",
    "\n",
    "tags_labels = np.array(['P', 'W', 'C', 'Q', 'Z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "893c64a7-9549-4bae-8a0b-5a373d818d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_array = preds[0][:19]\n",
    "tags_array = preds[0][19:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "412f15d7-4856-492b-9449-8fd427d8b291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "55d682ec-845d-49f4-9f31-cee4ec793c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74347ac6-eed1-4d15-a893-e88b711f06c1",
   "metadata": {},
   "source": [
    "### Intent Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b0ae37a2-fc19-4220-922c-a9bf4f0a413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent = intents_map[intents_array.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "285ff9e7-3b88-47e7-be03-826f6245392d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cancel_order'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce9f05-945b-4259-b040-7b3e46bee787",
   "metadata": {},
   "source": [
    "### Language Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "14f826fd-2158-4f2e-9f23-e466577031ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tag = \"\"\n",
    "\n",
    "for i in np.where(tags_array == 1)[0]:\n",
    "  output_tag += tags_labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5d330733-e9ca-404f-a96e-6749644d7583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc955b96-4626-43ac-a3a9-d2f0f4c4b08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
